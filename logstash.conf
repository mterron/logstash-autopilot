input {
	# SysLog (RFC 3164) 
	syslog {
		port => "${SYSLOG_PORT:3164}"
		type => "RFC3164"
		tags => ["syslog"]
	}
	# SysLog (RFC 5424)
	tcp {
		port => "${RFC5424_PORT:5424}"
		type => "RFC5424"
		tags => ["syslog"]
	}
	udp {
		port => "${RFC5424_PORT:5424}"
		type => "RFC5424"
		tags => ["syslog"]
	}
	# RELP
	# It is not a default input plugin! 
	relp {
		port => "${RELP_PORT:10514}"
		type => "RFC5424"
		tags => ["syslog"]
		#ssl_enable => false
		#ssl_cert => "/etc/tls/cert.pem"
		#ssl_key => "/etc/tls/cert.key"
		#ssl_cacert => "/etc/ssl/private/ca.pem"
		#ssl_verify => false
	}
	# Graylog2
	gelf {
		port => "${GELF_PORT:12201}"
		type => "gelf"
	}
	# JSON lines (one event per line, \n delimited stream)
	tcp {
		port  => "${JSONLINES_PORT:13000}"
		type  => "json"
		codec => "json_lines"
	}
	udp {
		port  => "${JSONLINES_PORT:13000}"
		type  => "json"
		codec => "json_lines"
	}
	# JSON documents
	tcp {
		port  => "${JSON_PORT:14000}"
		type  => "json"
		codec => "json"
	}
	udp {
		port  => "${JSON_PORT:14000}"
		type  => "json"
		codec  => "json"
	}
	# Fluent
	# Fluent is limited to a second precission
	# fluentd codec support is currently broken in Logstash
	# https://github.com/logstash-plugins/logstash-codec-fluent/issues/2
	# https://github.com/logstash-plugins/logstash-codec-fluent/pull/5
	#tcp {
	#	port  => "${FLUENT_PORT:24224}"
	#	type  => "fluent"
	#	codec => "fluent"
	#}
	#udp {
	#	port  => "${FLUENT_PORT:24224}"
	#	type  => "fluent"
	#	codec => "fluent"
	#}
	# Beats
	beats {
		port => "${BEATS_PORT:25109}"
		type => "beats"
		#ssl	 => false
		#ssl_certificate => "/etc/tls/cert.pem"
		#ssl_key => "/etc/tls/cert.key"
		#ssl_certificate_authorities => ["/etc/ssl/private/ca.pem"]
		#ssl_verify_mode => "force_peer"
	}
	# NMAP
	# It is not a default codec plugin!
	# Use: nmap -sP example.net -oX - | curl http://logstash-nmap.service.consul:6666 -d @-
	# See https://www.elastic.co/blog/using-nmap-logstash-to-gain-insight-into-your-network
	http {
		port  => "${NMAP_PORT:6666}"
		type  => "nmap"
		codec => "nmap"
	}
}

filter {
	if [type] == "RFC3164" {
	
	}
	else if [type] == "RFC5424" {
		grok {
			match	=> { "message"	=> "%{SYSLOG5424PRI}%{NONNEGINT}%{SPACE}+(?:%{TIMESTAMP_ISO8601:syslog_timestamp}|-)%{SPACE}+(?:%{HOSTNAME:hostname}|-)%{SPACE}+(?:%{NOTSPACE:app}|-)%{SPACE}+(?:%{NOTSPACE:pid}|-)%{SPACE}+(?:%{NOTSPACE:msg_id}|-)%{SPACE}+(?:%{SYSLOG5424SD:structured_data}|-|)%{SPACE}+%{GREEDYDATA:syslog_message}(?:(\\|\r|\n)+)" 
			}
			remove_field	=> "syslog5424_pri"
		}
	}
	#else if [type] == "gelf" {
	#	json {
	#	source	=>	"message"
	#	}
	#}
	else if [type] == "nmap" {
		# Don't emit documents for 'down' hosts
		if [status][state] == "down" {
			drop {}
		}

		mutate {
			# Drop HTTP headers and logstash server hostname
			remove_field => ["headers", "hostname"]
		}
	}
}

output {
	if "_grokparsefailure" in [tags] {
		file { path	=> "/usr/share/logstash/log/grok_failure_events-%{+YYYY-MM-dd}" }
		# DEBUG
		stdout { codec	=> "rubydebug" }
	} 
	else {
		if [type] == "nmap" {
			#elasticsearch {
			#	document_type => "%{[type]}"
			#	document_id => "%{[id]}"
			#	index => "nmap-logstash-%{+YYYY.MM}"
			#	template => "./elasticsearch_nmap_template.json"
			#	template_name => "logstash_nmap"
			# DEBUG
			stdout { codec	=> "rubydebug" }
		}
		else {
			elasticsearch { 
				hosts	=> [] 
				sniffing	=> true
			}
		}
	}
}